from airflow.decorators import dag, task
from pendulum import timezone
from scripts.azure_upload import upload_to_adls
from scripts.helpers import add_date_suffix
from datetime import datetime, timedelta

LOCAL_FILE_PATH = "/opt/airflow/data/ordenes.csv"
CONTAINER_NAME = "datalake" # airflow
WASB_CONN_ID = "utec_blob_storage"
BLOB_NAME = "raw/airflow/G3/ordenes.csv"

default_args = {
    'owner': 'airflow',
    'retries': 1,
    'retry_delay': timedelta(minutes=1),
}

@dag(
    dag_id="g3_utec",
    description="Uploads a local file to Azure Blob Storage with a date suffix.",
    default_args=default_args,
    start_date=datetime(2025, 1, 1, tzinfo=timezone("America/Bogota")),
    schedule="0 1 * * *", # Runs everyday at 01:00 a.m. local time (GMT-5)
    catchup=False,
    tags=["utec", "blob", "upload"],
)
def upload_dag():

    @task
    def call_upload():
        new_blob_name = add_date_suffix(BLOB_NAME)
        upload_to_adls(
            local_file_path=LOCAL_FILE_PATH,
            container_name=CONTAINER_NAME,
            blob_name=new_blob_name,
            wasb_conn_id = WASB_CONN_ID
            )

    call_upload()

dag = upload_dag()
